{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49840368-0fb3-4654-abae-8922780f555c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cvzone in c:\\users\\jaya verma\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\jaya verma\\anaconda3\\lib\\site-packages (from cvzone) (4.9.0.80)\n",
      "Requirement already satisfied: numpy in c:\\users\\jaya verma\\anaconda3\\lib\\site-packages (from cvzone) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install cvzone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9c0437-de4c-4c91-915c-668cba6c863a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaya verma\\anaconda3\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from math import hypot\n",
    "import numpy as np\n",
    "from ctypes import cast, POINTER\n",
    "from comtypes import CLSCTX_ALL\n",
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "import screen_brightness_control as sbc\n",
    "\n",
    "# Left Hand for Brightness, Right Hand for Volume\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mpHands = mp.solutions.hands\n",
    "hands = mpHands.Hands(min_detection_confidence=0.75)\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "\n",
    "devices = AudioUtilities.GetSpeakers()\n",
    "interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)\n",
    "volume = cast(interface, POINTER(IAudioEndpointVolume))\n",
    "\n",
    "volMin, volMax = volume.GetVolumeRange()[:2]\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "    img = cv2.flip(img, 1)\n",
    "    imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(imgRGB)\n",
    "\n",
    "    left_lmList, right_lmList = [], []\n",
    "    if results.multi_hand_landmarks and results.multi_handedness:\n",
    "        for i, hand_handedness in enumerate(results.multi_handedness):\n",
    "            label = MessageToDict(hand_handedness)['classification'][0]['label']\n",
    "            hand_landmarks = results.multi_hand_landmarks[i]\n",
    "            \n",
    "            lmList = []\n",
    "            h, w, _ = img.shape\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                lmList.append([int(lm.x * w), int(lm.y * h)])\n",
    "            \n",
    "            if label == 'Left':\n",
    "                left_lmList = lmList\n",
    "            elif label == 'Right':\n",
    "                right_lmList = lmList\n",
    "            \n",
    "            mpDraw.draw_landmarks(img, hand_landmarks, mpHands.HAND_CONNECTIONS)\n",
    "\n",
    "    if left_lmList:\n",
    "        x1, y1 = left_lmList[4][0], left_lmList[4][1]\n",
    "        x2, y2 = left_lmList[8][0], left_lmList[8][1]\n",
    "        cv2.line(img, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "        length = hypot(x2 - x1, y2 - y1)\n",
    "        bright = np.interp(length, [15, 200], [0, 100])\n",
    "        sbc.set_brightness(int(bright))\n",
    "\n",
    "    if right_lmList:\n",
    "        x1, y1 = right_lmList[4][0], right_lmList[4][1]\n",
    "        x2, y2 = right_lmList[8][0], right_lmList[8][1]\n",
    "        cv2.line(img, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "        length = hypot(x2 - x1, y2 - y1)\n",
    "        vol = np.interp(length, [15, 200], [volMin, volMax])\n",
    "        volume.SetMasterVolumeLevel(vol, None)\n",
    "\n",
    "    cv2.imshow('Image', img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6ccd70-9674-4809-bf4a-d85f33d20a44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
